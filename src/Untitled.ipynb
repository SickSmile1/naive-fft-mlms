{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee7eadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from math import isnan, sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as optim\n",
    "\n",
    "from NuMPI.Tools import Reduction\n",
    "from inspect import signature\n",
    "\n",
    "from SurfaceTopography import Topography\n",
    "from SurfaceTopography.Support import doi\n",
    "from SurfaceTopography import make_sphere\n",
    "import ContactMechanics as Solid\n",
    "from scipy.optimize import OptimizeResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8372b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_conjugate_gradients(fun, hessp, x0, gtol=1e-8,\n",
    "                                    mean_value=None, residual_plot=True,\n",
    "                                    maxiter=5000):\n",
    "    \"\"\"\n",
    "    Implementation of constrained conjugate gradient algorithm as described in,\n",
    "    I.A. Polonsky, L.M. Keer, Wear 231, 206 (1999).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fun : callable.\n",
    "                The objective function to be minimized.\n",
    "                            fun(x) -> float(energy),ndarray(gradient)\n",
    "                where x is the input ndarray.\n",
    "                Note that energy is never used, you can return a dummy value.\n",
    "\n",
    "    hessp : callable\n",
    "            Function to evaluate the hessian product of the objective.\n",
    "            Hessp should accept either 1 argument (desscent direction) or\n",
    "            2 arguments (x,descent direction).\n",
    "                            hessp(des_dir)->ndarray\n",
    "                                    or\n",
    "                            hessp(x,des_dir)->ndarray\n",
    "            where x is the input ndarray and des_dir is the descent direction.\n",
    "\n",
    "    x0 : ndarray\n",
    "         Initial guess.\n",
    "         ValueError is raised if \"None\" is provided.\n",
    "\n",
    "    gtol : float, optional\n",
    "           Default value : 1e-8\n",
    "           convergence criterion is max(abs) and norm2 of the projected\n",
    "           gradient < gtol.\n",
    "\n",
    "    mean_value :  float, optional\n",
    "               If you want to apply the mean_value constraint then provide an\n",
    "               float value to the mean_value.\n",
    "\n",
    "    residual_plot : bool, optional\n",
    "                    Generates a plot between the residual and iterations.\n",
    "\n",
    "    maxiter : int, optional\n",
    "              Default, maxiter=5000\n",
    "              Maximum number of iterations after which the program will exit.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OptimizeResult  : scipy.optimize object.\n",
    "        Attributes:\n",
    "         success: bool\n",
    "         x: x,\n",
    "         jac: residual = gradient(x),\n",
    "         nit: n_iterations,\n",
    "         message: 'CONVERGENCE: NORM_OF_GRADIENT_<=_GTOL' or 'NO CONVERGENCE: MAXITERATIONS REACHED'\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    ..[1] I.A. Polonsky, L.M. Keer, Wear 231, 206 (1999)\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(mean_value, (type(None), int, float)):\n",
    "        raise ValueError('Inappropriate type: {} for mean_value whereas a '\n",
    "                         'float \\\n",
    "            or int is expected'.format(type(mean_value)))\n",
    "\n",
    "    if not isinstance(residual_plot, bool):\n",
    "        raise ValueError('Inappropriate type: {} for \"residual_plot\" whereas '\n",
    "                         'a bool \\\n",
    "                         is expected'.format(type(residual_plot)))\n",
    "\n",
    "    \n",
    "    if x0 is not None:\n",
    "        x = x0.copy()\n",
    "        x = x.flatten()\n",
    "        delta = 0\n",
    "        G_old = 1\n",
    "    else:\n",
    "        raise ValueError('Input required for x0/initial value !!')\n",
    "    #fig, ax = plt.subplots()\n",
    "    #ax.plot(x, np.arange(x.size))\n",
    "    gaps = np.array([])\n",
    "    iterations = np.array([])\n",
    "\n",
    "    des_dir = np.zeros(np.shape(x))\n",
    "\n",
    "    if residual_plot:\n",
    "        gaps = np.append(gaps, 0)\n",
    "        iterations = np.append(iterations, 0)\n",
    "\n",
    "    n_iterations = 1\n",
    "    \n",
    "    while (n_iterations < maxiter + 1):\n",
    "\n",
    "        '''Mask to truncate the negative values'''\n",
    "        mask_neg = x <= 0\n",
    "        x[mask_neg] = 0.0\n",
    "\n",
    "        '''Initial residual or GAP'''\n",
    "        residual = fun(x)[1]\n",
    "\n",
    "        mask_c = x > 0\n",
    "        if mean_value is not None:\n",
    "            residual = residual - np.mean(residual[mask_c])\n",
    "\n",
    "        G = np.sum(residual[mask_c] ** 2)\n",
    "\n",
    "        des_dir[mask_c] = -residual[mask_c] + delta * (G / G_old) * des_dir[\n",
    "            mask_c]\n",
    "        des_dir[np.logical_not(mask_c)] = 0\n",
    "        G_old = G\n",
    "\n",
    "        '''Calculating step-length alpha'''\n",
    "\n",
    "        sig = signature(hessp)\n",
    "        if len(sig.parameters) == 2:\n",
    "            hessp_val = hessp(x, des_dir)\n",
    "        elif len(sig.parameters) == 1:\n",
    "            hessp_val = hessp(des_dir)\n",
    "        else:\n",
    "            raise ValueError('hessp function has to take max 1 arg (descent '\n",
    "                             'dir) or 2 args (x, descent direction)')\n",
    "\n",
    "        '''Here hessp_val is used as r_ij in original algorithm'''\n",
    "        if mean_value is not None:\n",
    "            hessp_val = hessp_val - np.mean(hessp_val[mask_c])\n",
    "\n",
    "        if mask_c.sum() != 0:\n",
    "            '''alpha is TAU from algorithm'''\n",
    "            alpha = -np.sum(residual[mask_c] * des_dir[mask_c]) / np.sum(hessp_val[mask_c] * des_dir[mask_c])\n",
    "        else:\n",
    "            # TODO: does anything happen when alpha is 0 or is the algorithm just stuck ?\n",
    "            alpha = 0.0\n",
    "\n",
    "        if alpha < 0:\n",
    "            print(\"it {} : hessian is negative along the descent direction. \"\n",
    "                  \"You will probably need linesearch \"\n",
    "                  \"or trust region\".format(n_iterations))\n",
    "\n",
    "        x[mask_c] += alpha * des_dir[mask_c]\n",
    "\n",
    "        '''mask for contact'''\n",
    "        mask_neg = x <= 0\n",
    "        '''truncating negative values'''\n",
    "        x[mask_neg] = 0.0\n",
    "\n",
    "        mask_g = residual < 0\n",
    "        mask_overlap = np.logical_and(mask_neg, mask_g)\n",
    "\n",
    "        if mask_overlap.sum() == 0:\n",
    "            delta = 1\n",
    "        else:\n",
    "            delta = 0\n",
    "            x[mask_overlap] = x[mask_overlap] - alpha * residual[mask_overlap]\n",
    "\n",
    "        if mean_value is not None:\n",
    "            '''Take care of constraint a_x*a_y*sum(p_ij)=P0'''\n",
    "            P = np.mean(x)\n",
    "            x *= (mean_value / P)\n",
    "\n",
    "        if residual_plot:\n",
    "            iterations = np.append(iterations, n_iterations)\n",
    "            if mask_c.sum() != 0:\n",
    "                gaps = np.append(gaps, np.max(abs(residual[mask_c])))\n",
    "            else:\n",
    "                gaps = np.append(gaps, np.max(abs(residual)))\n",
    "\n",
    "        n_iterations += 1\n",
    "        res_convg = False\n",
    "        assert np.logical_not(np.isnan(x).any())\n",
    "\n",
    "        if n_iterations >= 3:\n",
    "            '''If converged'''\n",
    "            if mask_c.sum() != 0:\n",
    "                if np.max(abs(residual[mask_c])) <= gtol:\n",
    "                    res_convg = True\n",
    "                else:\n",
    "                    res_convg = False\n",
    "\n",
    "            if res_convg:\n",
    "                result = OptimizeResult(\n",
    "                    {\n",
    "                        'success': True,\n",
    "                        'x': x,\n",
    "                        'jac': residual,\n",
    "                        'nit': n_iterations,\n",
    "                        'message': 'CONVERGENCE: NORM_OF_GRADIENT_<=_GTOL',\n",
    "                        })\n",
    "                if residual_plot:\n",
    "                    import matplotlib.pyplot as plt\n",
    "                    plt.plot(iterations, np.log10(gaps))\n",
    "                    plt.xlabel('iterations')\n",
    "                    plt.ylabel('residuals')\n",
    "                    plt.show()\n",
    "\n",
    "                return result\n",
    "\n",
    "            elif (n_iterations >= maxiter - 1):\n",
    "                '''If no convergence'''\n",
    "                result = OptimizeResult({\n",
    "                    'success': False,\n",
    "                    'x': x,\n",
    "                    'jac': residual,\n",
    "                    'nit': n_iterations,\n",
    "                    'message': 'NO-CONVERGENCE: MAXITERATIONS REACHED',\n",
    "                    })\n",
    "\n",
    "                if residual_plot:\n",
    "                    import matplotlib.pyplot as plt\n",
    "                    plt.plot(iterations, np.log10(gaps))\n",
    "                    plt.xlabel('iterations')\n",
    "                    plt.ylabel('residuals')\n",
    "                    plt.show()\n",
    "\n",
    "                return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "53e1dde9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for //: 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m----> 3\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43mMPI_Quadratic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpnp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mReduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m xstart \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(size\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mnb_subdomain_grid_pts)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(obj.hessian_product)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 17\u001b[0m, in \u001b[0;36mMPI_Quadratic.__init__\u001b[0;34m(self, nb_domain_grid_pts, pnp, factors, startpoint, xmin)\u001b[0m\n\u001b[1;32m     14\u001b[0m nprocs \u001b[38;5;241m=\u001b[39m comm\u001b[38;5;241m.\u001b[39mGet_size()\n\u001b[1;32m     15\u001b[0m rank \u001b[38;5;241m=\u001b[39m comm\u001b[38;5;241m.\u001b[39mGet_rank()\n\u001b[0;32m---> 17\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[43mnb_domain_grid_pts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m nprocs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdomain_slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(rank \u001b[38;5;241m*\u001b[39m step, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for //: 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "n = 128\n",
    "\n",
    "obj = MPI_Quadratic((n,n), pnp=Reduction(), )\n",
    "\n",
    "xstart = np.random.normal(size=obj.nb_subdomain_grid_pts)\n",
    "#print(obj.hessian_product)\n",
    "\n",
    "res = constrained_conjugate_gradients(\n",
    "    obj.f_grad,\n",
    "    obj.hessian_product,\n",
    "    x0=xstart,\n",
    ")\n",
    "\"\"\"assert res.success, res.message\n",
    "print(res.nit)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4106d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPI_Quadratic():\n",
    "    \"\"\"\n",
    "    n should be even\n",
    "\n",
    "    :param x: 1d array\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    bounds = (-4, 4)\n",
    "\n",
    "    def __init__(self, nb_domain_grid_pts, pnp=Reduction(), factors=None,\n",
    "                 startpoint=None, xmin=None):\n",
    "\n",
    "        comm = pnp.comm\n",
    "        nprocs = comm.Get_size()\n",
    "        rank = comm.Get_rank()\n",
    "\n",
    "        step = nb_domain_grid_pts // nprocs\n",
    "\n",
    "        if rank == nprocs - 1:\n",
    "            self.subdomain_slices = slice(rank * step, None)\n",
    "            self.subdomain_locations = rank * step\n",
    "            self.nb_subdomain_grid_pts = nb_domain_grid_pts - rank * step\n",
    "        else:\n",
    "            self.subdomain_slices = slice(rank * step, (rank + 1) * step)\n",
    "            self.subdomain_locations = rank * step\n",
    "            self.nb_subdomain_grid_pts = step\n",
    "\n",
    "        # helps to select the data that has odd or even index in the global\n",
    "        # array\n",
    "        self.pnp = pnp\n",
    "\n",
    "        if xmin is None:\n",
    "            self._xmin = np.zeros(self.nb_subdomain_grid_pts)\n",
    "        else:\n",
    "            self._xmin = xmin[self.subdomain_slices]\n",
    "\n",
    "        if factors is not None:\n",
    "            self.factors = factors[self.subdomain_slices]\n",
    "        else:\n",
    "            self.factors = np.random.random(self.nb_subdomain_grid_pts) + 0.1\n",
    "\n",
    "        if startpoint is not None:\n",
    "            self._startpoint = startpoint[self.subdomain_slices]\n",
    "        else:\n",
    "            self._startpoint = np.random.normal(\n",
    "                size=self.nb_subdomain_grid_pts)\n",
    "\n",
    "        self.nfeval = 0\n",
    "        self.ngradeval = 0\n",
    "\n",
    "    def f_grad(self, x):\n",
    "        self.nfeval += 1\n",
    "        self.ngradeval += 1\n",
    "        factdotx = self.factors.reshape(x.shape) \\\n",
    "            * (x - self._xmin.reshape(x.shape))\n",
    "        return self.pnp.sum(factdotx ** 2, axis=0).item(), 2 * factdotx\n",
    "\n",
    "    def f(self, x):\n",
    "        self.nfeval += 1\n",
    "        factdotx = self.factors.reshape(x.shape) \\\n",
    "            * (x - self._xmin.reshape(x.shape))\n",
    "        return self.pnp.sum(factdotx ** 2, axis=0).item()\n",
    "\n",
    "    def grad(self, x):\n",
    "        self.ngradeval += 1\n",
    "        return 2 * self.factors.reshape(x.shape) * (x - self._xmin)\n",
    "\n",
    "    def hessian_product(self, p):\n",
    "        return 2 * self.factors.reshape(p.shape) * p\n",
    "\n",
    "    def startpoint(self):\n",
    "        \"\"\"\n",
    "        standard starting point\n",
    "        :param n:\n",
    "        :return: array of shape (1,n)\n",
    "        \"\"\"\n",
    "        return self._startpoint\n",
    "\n",
    "    @staticmethod\n",
    "    def minVal(*args):\n",
    "        return 0\n",
    "\n",
    "    def xmin(self):\n",
    "        \"\"\"\n",
    "        Location of minimum according to\n",
    "        :param n: number of DOF\n",
    "        :return: array of size n\n",
    "        \"\"\"\n",
    "        return self._xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57eb77c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "def get_dtype_info(dtype):\n",
    "    if dtype.kind == 'i':\n",
    "        return np.iinfo(dtype)\n",
    "    if dtype.kind == 'f':\n",
    "        return np.finfo(dtype)\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "class Reduction:\n",
    "    def __init__(self, comm=None):\n",
    "        if comm is None:\n",
    "            self.comm = MPI.COMM_SELF\n",
    "        else:\n",
    "            self.comm = comm\n",
    "\n",
    "    def _op(self, npop, npargs, mpiop, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Generic reduction operation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        npop : func\n",
    "            Numpy reduction function (e.g. np.sum)\n",
    "        npargs: tuple\n",
    "            Arguments passed to the reduction function (e.g. array to be\n",
    "            reduced)\n",
    "        mpiop : mpi4py.MPI.op\n",
    "            MPI reduction operation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result_arr : np.ndarray\n",
    "            Result of the reduction operation\n",
    "        \"\"\"\n",
    "        local_result = npop(*npargs, *args, **kwargs)\n",
    "        result = np.zeros_like(local_result)\n",
    "        mpitype = MPI._typedict[local_result.dtype.char]\n",
    "        self.comm.Allreduce([local_result, mpitype], [result, mpitype], op=mpiop)\n",
    "        return result\n",
    "\n",
    "    def _op1(self, npop, arr, mpiop, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Generic reduction operation that takes a single (array) argument\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        npop : func\n",
    "            Numpy reduction function (e.g. np.sum)\n",
    "        arr : array_like\n",
    "            Numpy array containing the data to be reduced\n",
    "        mpiop : mpi4py.MPI.op\n",
    "            MPI reduction operation\n",
    "        initial : arr.dtype\n",
    "            Value to use if local array is empty\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result_arr : np.ndarray\n",
    "            Result of the reduction operation\n",
    "        \"\"\"\n",
    "        if 'initial' in kwargs and isinstance(arr, np.ma.MaskedArray):\n",
    "            # Max/min on masked array do not support `initial`\n",
    "            arr = arr.filled(kwargs['initial'])\n",
    "            del kwargs['initial']\n",
    "        return self._op(npop, (arr,), mpiop, *args, **kwargs)\n",
    "\n",
    "    def sum(self, arr, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Summation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : array_like\n",
    "            Numpy array containing the data to be reduced\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result_arr : np.ndarray\n",
    "            Sum of all elements of the array over all processors\n",
    "        \"\"\"\n",
    "        return self._op1(np.sum, arr, MPI.SUM, *args, **kwargs)\n",
    "\n",
    "    def max(self, arr, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Maximum value\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : array_like\n",
    "            Numpy array containing the data to be reduced\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result_arr : np.ndarray\n",
    "            Maximum of all elements of the array over all processors\n",
    "        \"\"\"\n",
    "        kwargs['initial'] = get_dtype_info(arr.dtype).min\n",
    "        return self._op1(np.max, arr, MPI.MAX, *args, **kwargs)\n",
    "\n",
    "    def min(self, arr, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Minimum value\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : array_like\n",
    "            Numpy array containing the data to be reduced\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result_arr : np.ndarray\n",
    "            Minimum of all elements of the array over all processors\n",
    "        \"\"\"\n",
    "        kwargs['initial'] = get_dtype_info(arr.dtype).max\n",
    "        return self._op1(np.min, arr, MPI.MIN, *args, **kwargs)\n",
    "\n",
    "    def mean(self, arr, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Arithmetic mean\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : array_like\n",
    "            Numpy array containing the data to be reduced\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result_arr : np.ndarray\n",
    "            Arithmetic mean of all elements of the array over all processors\n",
    "        \"\"\"\n",
    "        return self.sum(arr, *args, **kwargs) / self.sum(np.ones_like(arr), *args, **kwargs)\n",
    "\n",
    "    def dot(self, a, b, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Scalar product a.b\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : array_like\n",
    "            Numpy array containing the data of the first array\n",
    "        a : array_like\n",
    "            Numpy array containing the data of the second array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result_arr : np.ndarray\n",
    "            Scalar product between a and b\n",
    "        \"\"\"\n",
    "        return self._op(np.dot, (a, b), MPI.SUM, *args, **kwargs)\n",
    "\n",
    "    def any(self, arr, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns true of any value is true\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : array of bools\n",
    "            Input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result_arr : np.ndarray\n",
    "            True if any value in `arr` is true\n",
    "        \"\"\"\n",
    "        return self._op1(np.any, arr, MPI.LOR, *args, **kwargs)\n",
    "\n",
    "    def all(self, arr, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns true of all values are true\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : array of bools\n",
    "            Input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result_arr : np.ndarray\n",
    "            True if all values in `arr` are true\n",
    "        \"\"\"\n",
    "        return self._op1(np.all, arr, MPI.LAND, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ab603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
